{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86c3e9e6",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis \n",
    "\n",
    "We will analyze the transaction data to engineer features. We will try to answer a few questions using figures and tables enabling design of features, the fraud detection model, or the overall system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd36a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas \n",
    "!pip install numpy \n",
    "!pip install scikit-learn \n",
    "!pip install basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5644e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Display Properties\n",
    "from IPython.display import display, HTML\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 2)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40097f93",
   "metadata": {},
   "source": [
    "### Data Load\n",
    "\n",
    "Let's load the fraud data set first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5396fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/workspace/shared-data/transactions-1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4614c0e",
   "metadata": {},
   "source": [
    "## Format and Type\n",
    "\n",
    "The format of the file is CSV and below we see the various attributes and their data types as well take a sneak preview for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66718141",
   "metadata": {},
   "outputs": [],
   "source": [
    "## format and type\n",
    "display(HTML(df.dtypes.to_frame().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30182143",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0398c8",
   "metadata": {},
   "source": [
    "## Feature Statistics\n",
    "\n",
    "Determine the dynamics of each feature (int/float - math stats, text - categorical or not). We have <b>11</b> numeric columns out of which <b>is_fraud</b> is our target variable and so not a feature. So in effect we have 10 numeric features and 12 textual or categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1bff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first get the statistics of all the numeric columns\n",
    "math_stats = df.describe() \n",
    "display(math_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first get the statistics of all the categorical columns\n",
    "cat_stats = df.describe(include=[object])\n",
    "display(cat_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a993c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some unique values for some of the numeric attributes\n",
    "unique_cc_num = df['cc_num'].nunique() \n",
    "\n",
    "# This is to see if the combination of merchant latitude longitude add up to the merchant\n",
    "df[\"merchant_loc\"] = df[\"merch_lat\"].astype(str) + df[\"merch_long\"].astype(str)\n",
    "unique_merchant_lat_long = df['merchant_loc'].nunique() \n",
    "\n",
    "# This is to see if the combination of person latitude longitude add up to a person\n",
    "df[\"person_loc\"] = df[\"lat\"].astype(str) + df[\"long\"].astype(str)\n",
    "unique_person_lat_long = df['person_loc'].nunique() \n",
    "\n",
    "# This is to see if we can find unique individuals using first, last, sex and date of birth\n",
    "df[\"person\"] = df[\"dob\"].astype(str) + df[\"first\"] + df[\"last\"] + df[\"sex\"] + df[\"job\"]\n",
    "unique_person = df['person'].nunique() \n",
    "\n",
    "# This is to get unique addresses since fraudulent transactions could be occurring from a particular address\n",
    "df[\"address\"] = df[\"street\"] + df[\"city\"] + df[\"state\"] + df[\"zip\"].astype(str)\n",
    "unique_addresses = df['address'].nunique()\n",
    "\n",
    "# Get Unique States\n",
    "unique_states = df['state'].nunique()\n",
    "\n",
    "# Get Unique Categories\n",
    "unique_categories = df['category'].unique()\n",
    "\n",
    "# Get Unique Job\n",
    "unique_jobs = df['job'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d49e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stats = np.column_stack((unique_person, unique_cc_num, unique_addresses, \n",
    "                               unique_person_lat_long, unique_merchant_lat_long , unique_states, unique_jobs))\n",
    "unique_stats_df = pd.DataFrame(unique_stats, columns = ['Unique Customers','Unique Customer Ids',\n",
    "                                                        'Unique Addresses', 'Unique Customer Lat/Long',\n",
    "                                                        'Unique Merchant Lat/Long','Unique States', 'Unique Jobs'])\n",
    "display(HTML(unique_stats_df.to_html()))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f02831",
   "metadata": {},
   "source": [
    "From the above we can conclude that there are 999 unique individuals residing at a particular address since there are also 999 addresses. Including all these 3 features might not help us in improving the accuracy of any model. So we want to eliminate some of these features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b25f0c",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "We will clean up the data to cover the following \n",
    "\n",
    "* Find and List number of blank entries and outliers/errors\n",
    "* Take corrective actions and provide justification\n",
    "* Remove unnecessary features\n",
    "* Derive new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's copy the dataframe to keep a backup\n",
    "df_bak = df.copy(deep=True)\n",
    "\n",
    "# Then let's look at missing values \n",
    "display(HTML(df.isna().sum().to_frame().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bda082",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(df.isnull().sum().to_frame().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7299444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all the Date of birth values are valid dates \n",
    "all_dates_valid = pd.to_datetime(df['dob'], format='%Y-%m-%d', errors='coerce').notnull().all()\n",
    "\n",
    "# Check that there are no illogical dates \n",
    "dob_year_values = pd.to_datetime(df['dob'], format='%Y-%m-%d', errors='coerce').dt.year.values\n",
    "is_valid_year =  np.any((dob_year_values < 2019)|(dob_year_values > 1920 ))\n",
    "\n",
    "dob_stats = np.column_stack((all_dates_valid, is_valid_year ))\n",
    "dob_stats_df = pd.DataFrame(dob_stats, columns = ['Valid DOB?','Valid Year in DOB?'])\n",
    "display(HTML(dob_stats_df.to_html()))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e644a",
   "metadata": {},
   "source": [
    "If you see the above this dataset does not have any missing values. So we don't have any corrective action to take. So now let's see if we can reduce some of the features. We already can remove the following duplicate features since the information is available in some other attributes or we know the attribute is some sort of sequence generator having high cardinality that is not going to influence whether a transaction is a fraudulent one or not\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "         <b>first unnamed col: </b> This is just a sequence number for the data rows and so can be removed\n",
    "    </li>\n",
    "     <li>\n",
    "        <b>trans_date_trans_time: </b> This is just a timestamp and is <b>duplicate</b> to <b>unix_time</b>. Also the latter is numeric be easier to use\n",
    "    </li>\n",
    "     <li>\n",
    "        <b>trans_num: </b> This is just sequence or unique generated identifier assigned to every transaction\n",
    "     </li>  \n",
    "</ul>\n",
    "\n",
    "We will instead derive the following features\n",
    "\n",
    "<ol>\n",
    "    <li>\n",
    "        <b>Age:</b> Age from the Date of birth\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Day of the Week:</b> Day of the week for the transaction derived from Transaction Date\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Time of the Day:</b> Time of the Day for the transaction derived from Transaction Date\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Month:</b> Month for the transaction derived from Transaction Date\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d7f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date \n",
    "\n",
    "# Let's convert transaction date and time and dob to date-time\n",
    "df[\"dob_dt\"] = pd.to_datetime(df['dob'], format='%Y-%m-%d', errors='coerce')\n",
    "df[\"txn_dt\"] = pd.to_datetime(df['trans_date_trans_time'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "# Compute weekday from transaction date\n",
    "df['txn_weekday'] = df['txn_dt'].dt.day_name()\n",
    "\n",
    "# This function converts given date to age \n",
    "def age(born): \n",
    "    born = datetime.strptime(born, \"%Y-%m-%d\").date() \n",
    "    today = date.today() \n",
    "    return today.year - born.year - ((today.month,  today.day) < (born.month,  born.day)) \n",
    "\n",
    "# Compute age from date of birth \n",
    "df['age'] = df['dob'].apply(age) \n",
    "\n",
    "# Compute hour from transaction date\n",
    "df['txn_hour'] = df['txn_dt'].dt.hour\n",
    "\n",
    "# Slot the times into well known time ranges\n",
    "time_ranges = [0,4,8,12,16,21,24]\n",
    "part_of_day_dict = ['Late Night', 'Early Morning','Morning','Afternoon','Evening','Night']\n",
    "df['part_of_day'] = pd.cut(df['txn_hour'], bins=time_ranges, labels=part_of_day_dict, include_lowest=True)\n",
    "\n",
    "# Compute month from transaction date\n",
    "df['txn_month'] = df['txn_dt'].dt.month_name()\n",
    "\n",
    "def haversine_vectorize(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"Returns distance, in kilometers, between one set of longitude/latitude coordinates and another\"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    " \n",
    "    newlon = lon2 - lon1\n",
    "    newlat = lat2 - lat1\n",
    " \n",
    "    haver_formula = np.sin(newlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(newlon/2.0)**2\n",
    " \n",
    "    dist = 2 * np.arcsin(np.sqrt(haver_formula ))\n",
    "    miles = 3958 * dist #6367 for distance in KM \n",
    "    return miles\n",
    "\n",
    "df['distance_from_merchant'] = haversine_vectorize(df['lat'],df['long'],df['merch_lat'],df['merch_long'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d551b",
   "metadata": {},
   "source": [
    "## Analysis to answer Key Questions\n",
    "\n",
    "#### Distribution between Fraudulent and Non-Fraudulent Transactions\n",
    "\n",
    "We will leverage a pie chart to see the distribution and we can see that only 0.52% of the transactions are fraudulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.pie(df[\"is_fraud\"])\n",
    "plt.title('Percentage of Fraudulent Transactions')\n",
    "is_fraud_df = df.groupby(['is_fraud'])[\"is_fraud\"].count()\n",
    "\n",
    "labels = [r'Legit (99.48 %)', r'Fraud (0.52 %)']\n",
    "colors = ['yellowgreen', 'red']\n",
    "patches, texts = plt.pie(is_fraud_df, colors=colors,labels=[\"Legit\",\"Fraud\"])\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.legend(patches, labels, loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d13570",
   "metadata": {},
   "source": [
    "#### Distribution by Age, Day of the Week, Time of the Day and Month\n",
    "\n",
    "We want to answer the following questions\n",
    "\n",
    "<ol>\n",
    "    <li>\n",
    "        <b>Which age groups are more susceptible to fraudulent transactions?:</b> We can see that age does play some role where we see folks in age groups 30-60 are more susceptible to fraud\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>What is the distribution between time transactions between fraudulent transactions and non-fraudulent transactions? What is the most likely time of a fraudulent transaction?:</b> We do see that Fraud is more likely to happen Late Night or Night between 8 pm - 4 am\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>What is the distribution of fraudulent and non-fraudulent transactions occurring for each day of the week (i.e., Sunday, Monday, etc.), each month of the year (i.e., January, February)?:</b> There isn't a lot of variation between days of the week indicating fraud is likely overall\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Compared to any other time of the year, how prevalent are fraudulent transactions during the holidays (11-30 to 12-31)? During post-holidays (1-1 to 2-28)? During the summer (05-24 to 09-01)?:</b> We do see that Fraud is more likely to happen after major holidays such as Thanksgiving and Christmas because we see a lot in December and January\n",
    "    </li>  \n",
    "   \n",
    "</ol>\n",
    "\n",
    "We want to see if certain age groups are more susceptible to fraudulent transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the dataframe with fraudulent transactions\n",
    "fraud_df = df[df['is_fraud'] == 1] \n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(28, 8))\n",
    "\n",
    "ax1.hist(fraud_df['age'])\n",
    "ax1.set_title('Distribution by Age')\n",
    "ax1.set_ylabel('Age')\n",
    "\n",
    "ax2.hist(fraud_df['txn_weekday'],color = \"orange\")\n",
    "ax2.set_title('Distribution by Weekday')\n",
    "ax2.set_ylabel('Weekday')\n",
    "ax2.set_xticklabels(fraud_df['txn_weekday'], rotation=90)\n",
    "\n",
    "ax3.hist(fraud_df['part_of_day'],color = \"green\")\n",
    "ax3.set_title('Distribution by Time of Day')\n",
    "ax3.set_ylabel('Time of Day')\n",
    "\n",
    "ax4.hist(fraud_df['txn_month'],color = \"red\")\n",
    "ax4.set_title('Distribution by Month')\n",
    "ax4.set_ylabel('Month')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d6335b",
   "metadata": {},
   "source": [
    "We will break category down into specific indicators such as \n",
    "\n",
    "<ol>\n",
    "    <li>\n",
    "        <b>Internet Transaction</b>: Yes (1) or No (0) based on '_net' in the category\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Type</b>: 'Shopping' based on 'shopping_' in the category or if its groceries; 'Travel' based on 'gas_transport' or 'travel' in the category; 'Home' based on 'personal_care' or 'health_fitness' or 'kids_pets' or 'home' in the category; 'Entertainment' based on 'entertainment' or 'food_dining' in the category\n",
    "    </li>   \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c9f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_txn_internet(_category): \n",
    "    if (_category.endswith('_net')):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def normalize_category(_category): \n",
    "    if ((_category.find('shopping_')!=-1) | (_category.find('grocery_')!=-1)):\n",
    "        return 'Shopping'\n",
    "    elif ((_category.find('personal_care')!=-1) | (_category.find('health_fitness')!=-1) \n",
    "          | (_category.find('home')!=-1) | (_category.find('kids_pets')!=-1)):\n",
    "        return 'Home'\n",
    "    elif ((_category.find('entertainment')!=-1) | (_category.find('food_dining')!=-1) | \n",
    "         (_category.find('gas_')!=-1) | (_category.find('travel')!=-1)):\n",
    "        return 'Entertainment'\n",
    "    else:\n",
    "        return 'Misc'\n",
    "\n",
    "df['is_internet'] = df.apply(lambda x: is_txn_internet(x['category']),axis=1)\n",
    "df['normalized_category'] = df.apply(lambda x: normalize_category(x['category']),axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_num_counts = df.groupby(['normalized_category','is_fraud'])['is_fraud'].count() \n",
    "display(HTML(cat_num_counts.to_frame().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bfb89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_num_counts = df.groupby(['is_internet','is_fraud'])['is_fraud'].count() \n",
    "display(HTML(net_num_counts.to_frame().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7184f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install basemap-data-hires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc5527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# Obtain the latitude and longitude of the merchant where the transaction occured \n",
    "lat = fraud_df['merch_lat'].values\n",
    "lon = fraud_df['merch_long'].values\n",
    "\n",
    "# determine range to print based on min, max lat and lon of the data\n",
    "margin = 2 # buffer to add to the range\n",
    "lat_min = min(lat) - margin\n",
    "lat_max = max(lat) + margin\n",
    "lon_min = min(lon) - margin\n",
    "lon_max = max(lon) + margin\n",
    "\n",
    "# create map using BASEMAP\n",
    "m = Basemap(llcrnrlon=lon_min,\n",
    "            llcrnrlat=lat_min,\n",
    "            urcrnrlon=lon_max,\n",
    "            urcrnrlat=lat_max,\n",
    "            lat_0=(lat_max - lat_min)/2,\n",
    "            lon_0=(lon_max-lon_min)/2,\n",
    "            projection='merc',\n",
    "            resolution = 'h',\n",
    "            area_thresh=10000.,\n",
    "            )\n",
    "m.drawcoastlines()\n",
    "m.drawcountries()\n",
    "m.drawstates()\n",
    "m.drawmapboundary(fill_color='#46bcec')\n",
    "m.fillcontinents(color = 'white',lake_color='#46bcec')\n",
    "# convert lat and lon to map projection coordinates\n",
    "lons, lats = m(lon, lat)\n",
    "# plot points as red dots\n",
    "m.scatter(lons, lats, marker = 'o', color='r', zorder=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf58fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = fraud_df.groupby(['state'])['state'].count().to_dict()\n",
    "state_pop_dict = fraud_df.groupby(['state'])['city_pop'].sum().to_dict()\n",
    "state_final_dict = {}\n",
    "for key in state_dict.keys():\n",
    "    val = state_dict[key]\n",
    "    pop_val = state_pop_dict[key]\n",
    "    state_final_dict[key] = (val / pop_val)*10000\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f21320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import rgb2hex, Normalize\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Lambert Conformal map of lower 48 states.\n",
    "m = Basemap(llcrnrlon=-119,llcrnrlat=22,urcrnrlon=-64,urcrnrlat=49,\n",
    "        projection='lcc',lat_1=33,lat_2=45,lon_0=-95)\n",
    "\n",
    "# Mercator projection, for Alaska and Hawaii\n",
    "m_ = Basemap(llcrnrlon=-190,llcrnrlat=20,urcrnrlon=-143,urcrnrlat=46,\n",
    "            projection='merc',lat_ts=20)  # do not change these numbers\n",
    "\n",
    "#%% ---------   draw state boundaries  ----------------------------------------\n",
    "## data from U.S Census Bureau\n",
    "## http://www.census.gov/geo/www/cob/st2000.html\n",
    "shp_info = m.readshapefile('st99_d00','states',drawbounds=True,\n",
    "                           linewidth=0.45,color='gray')\n",
    "shp_info_ = m_.readshapefile('st99_d00','states',drawbounds=False)\n",
    "\n",
    "short_state_names = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "}\n",
    "\n",
    "#%% -------- choose a color for each state based on population density. -------\n",
    "colors={}\n",
    "statenames=[]\n",
    "cmap = plt.cm.hot_r # use 'reversed hot' colormap\n",
    "vmin = 0; vmax = 15 # set range.\n",
    "norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "for shapedict in m.states_info:\n",
    "    statename = shapedict['NAME']\n",
    "    short_name = list(short_state_names.keys())[list(short_state_names.values()).index(statename)]\n",
    "\n",
    "    # skip DC and Puerto Rico.\n",
    "    if statename not in ['District of Columbia','Puerto Rico']:\n",
    "        fraud_ct = state_final_dict[short_name]\n",
    "        # calling colormap with value between 0 and 1 returns\n",
    "        # rgba value.  Invert color range (hot colors are high\n",
    "        # fraud), take sqrt root to spread out colors more.\n",
    "        colors[statename] = cmap(np.sqrt((fraud_ct-vmin)/(vmax-vmin)))[:3]\n",
    "    statenames.append(statename)\n",
    "    \n",
    "#%% ---------  cycle through state names, color each one.  --------------------\n",
    "for nshape,seg in enumerate(m.states):\n",
    "    # skip DC and Puerto Rico.\n",
    "    if statenames[nshape] not in ['Puerto Rico', 'District of Columbia']:\n",
    "        color = rgb2hex(colors[statenames[nshape]])\n",
    "        poly = Polygon(seg,facecolor=color,edgecolor=color)\n",
    "        ax.add_patch(poly)\n",
    "\n",
    "AREA_1 = 0.005  # exclude small Hawaiian islands that are smaller than AREA_1\n",
    "AREA_2 = AREA_1 * 30.0  # exclude Alaskan islands that are smaller than AREA_2\n",
    "AK_SCALE = 0.19  # scale down Alaska to show as a map inset\n",
    "HI_OFFSET_X = -1900000  # X coordinate offset amount to move Hawaii \"beneath\" Texas\n",
    "HI_OFFSET_Y = 250000    # similar to above: Y offset for Hawaii\n",
    "AK_OFFSET_X = -250000   # X offset for Alaska (These four values are obtained\n",
    "AK_OFFSET_Y = -750000   # via manual trial and error, thus changing them is not recommended.)\n",
    "\n",
    "for nshape, shapedict in enumerate(m_.states_info):  # plot Alaska and Hawaii as map insets\n",
    "    if shapedict['NAME'] in ['Alaska', 'Hawaii']:\n",
    "        seg = m_.states[int(shapedict['SHAPENUM'] - 1)]\n",
    "        if shapedict['NAME'] == 'Hawaii' and float(shapedict['AREA']) > AREA_1:\n",
    "            seg = [(x + HI_OFFSET_X, y + HI_OFFSET_Y) for x, y in seg]\n",
    "            color = rgb2hex(colors[statenames[nshape]])\n",
    "        elif shapedict['NAME'] == 'Alaska' and float(shapedict['AREA']) > AREA_2:\n",
    "            seg = [(x*AK_SCALE + AK_OFFSET_X, y*AK_SCALE + AK_OFFSET_Y)\\\n",
    "                   for x, y in seg]\n",
    "            color = rgb2hex(colors[statenames[nshape]])\n",
    "        poly = Polygon(seg, facecolor=color, edgecolor='gray', linewidth=.45)\n",
    "        ax.add_patch(poly)\n",
    "\n",
    "ax.set_title('Fraudulent Transactions by state')\n",
    "\n",
    "#%% ---------  Plot bounding boxes for Alaska and Hawaii insets  --------------\n",
    "light_gray = [0.8]*3  # define light gray color RGB\n",
    "x1,y1 = m_([-190,-183,-180,-180,-175,-171,-171],[29,29,26,26,26,22,20])\n",
    "x2,y2 = m_([-180,-180,-177],[26,23,20])  # these numbers are fine-tuned manually\n",
    "m_.plot(x1,y1,color=light_gray,linewidth=0.8)  # do not change them drastically\n",
    "m_.plot(x2,y2,color=light_gray,linewidth=0.8)\n",
    "\n",
    "#%% ---------   Show color bar  ---------------------------------------\n",
    "ax_c = fig.add_axes([0.9, 0.1, 0.03, 0.8])\n",
    "cb = ColorbarBase(ax_c,cmap=cmap,norm=norm,orientation='vertical',\n",
    "                  label=r'[Fraudulent Transactions relative to Population]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da16b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_counts = df.groupby(['job','is_fraud'])['is_fraud'].count() \n",
    "display(HTML(job_counts.to_frame().to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56ad96f",
   "metadata": {},
   "source": [
    "From the above analysis (and some charts not shown here but covered in Preliminary Data Analysis) we see that fraudulent transactions are spread across customers and since the customer's atributes (first, last, dob, sex, job) are part of their identity these are also not likely to have any impact. Additionally from the unique counts above, we can conclude that there are 999 unique individuals residing at a particular address since there are also 999 addresses. Including both of these features might not help us in improving the accuracy of any model. So we can pick one set and drop the rest. So with this we will drop the following attributes and simply keep the <b>address</b>\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>first</b> \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>last</b> \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>sex</b> \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>dob</b> \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>person</b> \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>street</b> \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>city</b> \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>state</b> \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>zip</b> \n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "We can also remove these columns as we have come with a normalized definition or new derived features\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>lat</b> \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>long</b> \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>merch_lat</b> \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>merch_long</b> \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>category</b> \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>merchant</b> \n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c6abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_job(_job): \n",
    "    if ((_job.lower().find('research')!=-1) | (_job.lower().find('lab')!=-1)):\n",
    "        return 'Research'\n",
    "    elif ((_job.lower().find('academic')!=-1) | (_job.lower().find('teacher')!=-1)| \n",
    "          (_job.lower().find('education')!=-1)):\n",
    "        return 'Education'\n",
    "    elif ((_job.lower().find('engineer')!=-1) | (_job.lower().find('Engineering')!=-1) ):\n",
    "        return 'Engineering'\n",
    "    elif ((_job.lower().find('it')!=-1) | (_job.lower().find('software')!=-1) | (_job.lower().find('developer')!=-1) \n",
    "          | (_job.lower().find('programmer')!=-1) | (_job.lower().find('tech')!=-1)):\n",
    "        return 'Software'\n",
    "    elif ((_job.lower().find('psycho')!=-1) | (_job.lower().find('clinic')!=-1) | (_job.lower().find('health')!=-1)\n",
    "         | (_job.lower().find('therap')!=-1) | (_job.lower().find('surgeon')!=-1)):\n",
    "        return 'Health'\n",
    "    elif ((_job.lower().find('officer')!=-1) | (_job.lower().find('manager')!=-1) | (_job.lower().find('advis')!=-1)\n",
    "         | (_job.lower().find('bank')!=-1) | (_job.lower().find('executive')!=-1) | (_job.lower().find('staff')!=-1)\n",
    "         | (_job.lower().find('legal')!=-1) | (_job.lower().find('attorney')!=-1) | (_job.lower().find('sales')!=-1)):\n",
    "        return 'Officer'\n",
    "    elif ((_job.lower().find('desig')!=-1) | (_job.lower().find('media')!=-1) | (_job.lower().find('video')!=-1)\n",
    "         | (_job.lower().find('film')!=-1) | (_job.lower().find('television')!=-1)):\n",
    "        return 'Art'\n",
    "    else:\n",
    "        return 'Misc'\n",
    "    \n",
    "df['normalized_job'] = df.apply(lambda x: normalize_job(x['job']),axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1630aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_num_counts = df.groupby(['normalized_job','is_fraud'])['is_fraud'].count() \n",
    "display(HTML(job_num_counts.to_frame().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fdaa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the above columns\n",
    "cols_to_drop = ['cc_num','person','first','last','dob','sex','street','city','state','zip','person_loc',\n",
    "                'merchant_loc','lat','long','trans_num','trans_date_trans_time','unix_time',\n",
    "                'dob_dt','txn_dt','txn_hour','address','category','merch_lat','merch_long','normalized_job', 'merchant']\n",
    "trimmed_df = df.drop(columns=cols_to_drop,errors='ignore')\n",
    "\n",
    "# Drop the column with the serial number\n",
    "for col in trimmed_df.columns:\n",
    "    idx = df.columns.get_loc(col)\n",
    "    if idx == 0:\n",
    "        trimmed_df = trimmed_df.drop(columns=df.columns[idx])\n",
    "    \n",
    "trimmed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea68bdb",
   "metadata": {},
   "source": [
    "## Normalize\n",
    "Dont worry about text features but you must normalize the numeric features. \n",
    "* Provide rationale as to why the particular normalization feature was selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a01e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Calculate the z-score for all our key numeric attributes\n",
    "z1 = np.abs(stats.zscore(df['distance_from_merchant']))\n",
    "z2 = np.abs(stats.zscore(df['amt']))\n",
    "z3 = np.abs(stats.zscore(df['city_pop']))\n",
    "\n",
    "# Identify outlier percentages for all of these\n",
    "threshold = 3\n",
    "\n",
    "outliers1 = df[z1 > threshold]\n",
    "outliers2 = df[z2 > threshold]\n",
    "outliers3 = df[z3 > threshold]\n",
    "\n",
    "print((len(outliers1)/len(df))*100)\n",
    "print((len(outliers2)/len(df))*100)\n",
    "print((len(outliers3)/len(df))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cdd6d4",
   "metadata": {},
   "source": [
    "From the above the outlier percentages are relatively small for all columns hence for all of these MinMaxNormalization which puts the normalized value within the [0,1] range is a good choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ec6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the numeric features\n",
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "trimmed_df[['normalized_amt']] = min_max_scaler.fit_transform(trimmed_df[['amt']])\n",
    "trimmed_df[['normalized_city_pop']] = min_max_scaler.fit_transform(trimmed_df[['city_pop']])\n",
    "trimmed_df[['normalized_age']] = min_max_scaler.fit_transform(trimmed_df[['age']])\n",
    "trimmed_df[['normalized_distance_from_merchant']] = min_max_scaler.fit_transform(trimmed_df[['distance_from_merchant']])\n",
    "\n",
    "trimmed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af701f1",
   "metadata": {},
   "source": [
    "Since we have the normalized values we can now drop the original columns where we have taken normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc66920",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['amt','city_pop','age','distance_from_merchant']\n",
    "trimmed_df = trimmed_df.drop(columns=cols_to_drop,errors='ignore')\n",
    "trimmed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63fea48",
   "metadata": {},
   "source": [
    "While the problem does not ask for it at the moment since ML algorithms want numeric data we will also convert the text data from above into numeric values. Since the max values for any of these columns is 999 (for job) we will use label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the transformation on a copy of the dataframe\n",
    "numeric_df = trimmed_df[['normalized_amt','normalized_city_pop','normalized_age','normalized_distance_from_merchant']].copy()\n",
    "numeric_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a0a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df = trimmed_df[['is_fraud']].copy()\n",
    "categorical_df = trimmed_df[['is_internet','normalized_category','job','txn_weekday','txn_month','part_of_day']].copy()\n",
    "categorical_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc432de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to category\n",
    "ohc_category = OneHotEncoder()\n",
    "ohe_category= ohc_category.fit_transform(categorical_df['normalized_category'].values.reshape(-1,1)).toarray()\n",
    "df_category = pd.DataFrame(ohe_category, columns = ohc_category.categories_[0])\n",
    "df_category.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c645d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Fix the order for body types\n",
    "ordered_part_of_day = ['Late Night','Early Morning', 'Morning','Afternoon','Evening','Night']\n",
    "\n",
    "# Create the Ordinal Encoder\n",
    "oe_daypart = OrdinalEncoder(categories=[ordered_part_of_day])\n",
    "\n",
    "part_of_day_df = categorical_df[['part_of_day']].copy()\n",
    "part_of_day_df[['part_of_day']] = oe_daypart.fit_transform(part_of_day_df[['part_of_day']])\n",
    "part_of_day_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162a1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the order for months\n",
    "ordered_month = ['January','February', 'March','April','May','June','July','August','September',\n",
    "                      'October','November','December']\n",
    "\n",
    "# Create the Ordinal Encoder\n",
    "oe_month = OrdinalEncoder(categories=[ordered_month])\n",
    "\n",
    "month_df = categorical_df[['txn_month']].copy()\n",
    "month_df[['txn_month']] = oe_month.fit_transform(month_df[['txn_month']])\n",
    "month_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c476da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the order for months\n",
    "ordered_weekday = ['Sunday','Monday', 'Tuesday','Wednesday','Thursday','Friday','Saturday']\n",
    "\n",
    "# Create the Ordinal Encoder\n",
    "oe_weekday = OrdinalEncoder(categories=[ordered_weekday])\n",
    "\n",
    "weekday_df = categorical_df[['txn_weekday']].copy()\n",
    "weekday_df[['txn_weekday']] = oe_weekday.fit_transform(weekday_df[['txn_weekday']])\n",
    "weekday_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create instance of labelencoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "job_df = categorical_df[['job']].copy()\n",
    "job_df['job_enc'] = labelencoder.fit_transform(job_df['job'])\n",
    "\n",
    "cols_to_drop = ['job']\n",
    "job_df = job_df.drop(columns=cols_to_drop,errors='ignore')\n",
    "job_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c06bd",
   "metadata": {},
   "source": [
    "## Feature and Label Selection\n",
    "Down select from your data, the input features and label(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e8475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the dataframes\n",
    "fraud_features_df = pd.concat([job_df, df_category, weekday_df, month_df, part_of_day_df, numeric_df], axis=1)\n",
    "fraud_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d6cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the above we now have our key features which is all columns minus is_fraud and this will be X \n",
    "X = fraud_features_df.values\n",
    "y = class_df.values.ravel()\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0621ad64",
   "metadata": {},
   "source": [
    "## Split into 3 data sets for training, validation, and test (Explain your % for each)\n",
    "\n",
    "We are going to make use of train_test_split twice to get the Training, Validation and Test Datasets. The default values for this function is 75% training and 25% Testing. We will go with 80 and 20 instead just because we need a larger percentage in training since we will split it further to get our validation. \n",
    "\n",
    "We will also use stratified=y because this dataset is highly unbalanced i.e. the fraudulent transactions are small percentage of the overall transactions and hence we want to make sure that proporition is maintained in each of our splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get the range \n",
    "x_ids = list(range(len(X)))\n",
    "\n",
    "# Obtain training and test dataset \n",
    "x_train_ids, x_val_ids, y_train,y_val  = train_test_split(x_ids, y , test_size = 0.3, stratify=y, random_state=0)\n",
    "\n",
    "# Obtain training and validation dataset\n",
    "x_val_ids, x_test_ids, y_val, y_test = train_test_split(x_val_ids, y_val , test_size = 0.5, stratify=y_val, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9444f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now extract using the indices\n",
    "X_train = X[x_train_ids]\n",
    "X_val = X[x_val_ids]\n",
    "X_test = X[x_test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a451b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#pipe_lr = make_pipeline(RandomForestClassifier(warm_start=True, max_depth=11, n_estimators=100, max_features=12, random_state=42))\n",
    "rf_clf = RandomForestClassifier(warm_start=True, max_depth=11, n_estimators=100, max_features=12, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Increase estimators and add more data\n",
    "rf_clf.n_estimators += 100\n",
    "rf_clf.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f8863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Metric Arrays \n",
    "acc,precision, recall, f1, roc_auc = [], [], [], [], []\n",
    "\n",
    "# Measure on Training Data\n",
    "y_pred = rf_clf.predict(X_train)\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "\n",
    "precision       += [precision_score(y_train, y_pred)]\n",
    "recall          += [recall_score(y_train, y_pred)]\n",
    "f1              += [f1_score(y_train, y_pred)]\n",
    "roc_auc         += [roc_auc_score(y_train, y_pred)]\n",
    "acc             += [pipe_lr.score(X_train, y_train)]\n",
    "\n",
    "# Measure on Validation Data\n",
    "y_pred = rf_clf.predict(X_val)\n",
    "tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "\n",
    "precision       += [precision_score(y_val, y_pred)]\n",
    "recall          += [recall_score(y_val, y_pred)]\n",
    "f1              += [f1_score(y_val, y_pred)]\n",
    "roc_auc         += [roc_auc_score(y_val, y_pred)]\n",
    "acc             += [pipe_lr.score(X_val, y_val)]\n",
    "\n",
    "# Measure on Test Data\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "precision       += [precision_score(y_test, y_pred)]\n",
    "recall          += [recall_score(y_test, y_pred)]\n",
    "f1              += [f1_score(y_test, y_pred)]\n",
    "roc_auc         += [roc_auc_score(y_test, y_pred)]\n",
    "acc             += [pipe_lr.score(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef57a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, max_features=12, learning_rate=0.1,max_depth=11, random_state=42)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Increase estimators and add more data\n",
    "gb_clf.n_estimators += 100\n",
    "gb_clf.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d89f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure on Training Data\n",
    "y_pred = gb_clf.predict(X_train)\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, y_pred).ravel()\n",
    "\n",
    "precision       += [precision_score(y_train, y_pred)]\n",
    "recall          += [recall_score(y_train, y_pred)]\n",
    "f1              += [f1_score(y_train, y_pred)]\n",
    "roc_auc         += [roc_auc_score(y_train, y_pred)]\n",
    "acc             += [pipe_lr.score(X_train, y_train)]\n",
    "\n",
    "# Measure on Validation Data\n",
    "y_pred = gb_clf.predict(X_val)\n",
    "tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "\n",
    "precision       += [precision_score(y_val, y_pred)]\n",
    "recall          += [recall_score(y_val, y_pred)]\n",
    "f1              += [f1_score(y_val, y_pred)]\n",
    "roc_auc         += [roc_auc_score(y_val, y_pred)]\n",
    "acc             += [pipe_lr.score(X_val, y_val)]\n",
    "\n",
    "# Measure on Test Data\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "precision       += [precision_score(y_test, y_pred)]\n",
    "recall          += [recall_score(y_test, y_pred)]\n",
    "f1              += [f1_score(y_test, y_pred)]\n",
    "roc_auc         += [roc_auc_score(y_test, y_pred)]\n",
    "acc             += [pipe_lr.score(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a0ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all the metrics by classifier to compare\n",
    "classifiers = ['Random Forest', '', '', 'Gradient Boosted', '','']\n",
    "metrics_datasets = ['Training', 'Validation', 'Test','Training', 'Validation', 'Test']\n",
    "\n",
    "model_stats = np.column_stack((classifiers, metrics_datasets, acc, f1, roc_auc, precision, recall))\n",
    "model_stats_df = pd.DataFrame(model_stats, columns = ['Classifier','Dataset','Accuracy','F1 Score',\n",
    "                                                        'ROC AUC Score', 'Precision', 'Recall'])\n",
    "display(HTML(model_stats_df.to_html()))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9a904d",
   "metadata": {},
   "source": [
    "# Summary & Quality Check\n",
    "\n",
    "Overall this data had sufficient information in terms of number of customers, geographical spread, types of merchants & categories and proportion of fraudulent data vs valid data across all these dimensions. The data had extremely good quality and hence it is good for machine learning ingestion. Using stratified split of training, validation and test data we are able to get a 60, 20 & 20 split for training, validation and testing maintaining the 0.52% ratio of fraudulent transactions. \n",
    "\n",
    "<ul>\n",
    "     <li>\n",
    "         <b>Overall Quality of the data: </b> None of the columns had any missing or null values. The dates in Date of Birth were valid. Statistically the States and Zip codes are also valid. So overall the quality of the data is good\n",
    "    </li>\n",
    "    <li>\n",
    "         <b>Sufficient amount of the data: </b> There are 1.85M records and so there is sufficient amount of data in terms of rows\n",
    "    </li>\n",
    "    <li>\n",
    "         <b>Sparseness of any data categories (eg. no young adults): </b> This a very unbalanced dataset when looked at from a target variable stand point. There are only 0.5% fraudulent transactions. Having said that this is function of the domain i.e. in general the fraudulent transactions form a small percentage of the overall transactions and so it is not uncommon to see this. Within this data we see a good distribution of data amongst geos (states, zip codes, etc), customers, merchants and categories. We use the histograms to arrive at this conclusion. \n",
    "    </li>\n",
    "    <li>\n",
    "         <b>Trustworthiness of the data (Is it true?): </b> This is something we cannot verify and hence the biggest cons of this data. Given that the data has no missing values, does not have any outliers makes this suspect - it is likely that this is synthentic data. Real world data often has missing values, outliers, skewed distributions, etc \n",
    "    </li>\n",
    "    <li>\n",
    "         <b>Timeliness of the data (is it recent?)  What might be the problem if it is not?: </b> Given we are in 2024 this data is now a bit dated since it covers period between 2019 and 2020. Fraudulent actors are constantly changing how they commit fraud and so if we base our model on dated data we are not going to see it work well in the field. \n",
    "    </li>\n",
    "    <li>\n",
    "         <b>Note difficenties: </b> There were duplicate pieces of information such as transaction date time as well as unix timestamps as well as customer identification number and the pieces of information that can identify a customer uniquely. \n",
    "    </li>\n",
    "    <li>\n",
    "         <b>Available document on the data types, how the data was collected, how it was verified?: </b> The data was obtained from the Assignment Module 2 Content https://jhu.instructure.com/courses/66217/pages/case-study-transaction-fraud-detection?module_item_id=3574620. The description of the fields here was well explained and enabled removing redundant pieces of information. The only challenge is understanding which features (after removing rendundant columns) are still likely to be relevant. For example are elements of a customer's identity (E.g. Sex, DOB, Job), Address (Zip, State, City, Street), Where the transaction occured at the merchant (Lat, Long) might be influencing the chances of the transaction being fraudulent\n",
    "    </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd6935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
