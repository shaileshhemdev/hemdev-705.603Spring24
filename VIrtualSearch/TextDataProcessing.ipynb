{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d19a31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text Data Processing\n",
    "\n",
    "In this assignment we are writing the following 2 functions for Text Data Processing\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>preprocess:</b> Function takes in a pandas.Series() of a corpus of text data as an argument. This function should output an indexed vocabulary and preprocessed tokens.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>encode():</b> Function that takes in two arguments: 1) a pandas.Series() (or the preprocessed token outputs of the preprocess() function), and 2) a specified encoding method. These encoding methods must include Bag-of-Words, TF-IDF, and Word2Vec. \n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07c7ebcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (24.0)\n",
      "Requirement already satisfied: nltk in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: click in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: contractions in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "Requirement already satisfied: inflect in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (7.2.0)\n",
      "Requirement already satisfied: more-itertools in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from inflect) (10.2.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from inflect) (4.2.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from inflect) (4.11.0)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from typeguard>=4.0.1->inflect) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=3.6->typeguard>=4.0.1->inflect) (3.8.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: gensim in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n",
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n",
      "Requirement already satisfied: transformers in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (4.40.0)\n",
      "Requirement already satisfied: filelock in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shaileshhemdev/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install nltk\n",
    "!pip install contractions\n",
    "!pip install inflect\n",
    "!pip install scikit-learn \n",
    "!pip install gensim\n",
    "!pip uninstall -y tensorflow\n",
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49afd262-2b5f-4524-9099-86cb2e2cff51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.13\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa69683-bf75-4c4b-98ea-c42b4a4c3c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'negative', 'score': 0.7479290962219238}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Specify the model\n",
    "model_id = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=model_id)\n",
    "print(sentiment_pipe('I hate it'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5496449-72c0-4ecc-9299-f8cc2398d891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'negative', 'score': 0.49786096811294556}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_pipe('I would avoid it'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6583f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Display Properties\n",
    "from IPython.display import display, HTML\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 2)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db5be5ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shaileshhemdev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shaileshhemdev/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shaileshhemdev/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/shaileshhemdev/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import inflect\n",
    "import contractions\n",
    "from data_pipeline import Text_Pipeline\n",
    "\n",
    "# Download the various \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialize various tools\n",
    "text_pipeline = Text_Pipeline('CONVERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baf4c668",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                        quick brown fox jump lazy dog\n",
      "1                      king strength also include ally\n",
      "2                                 history write victor\n",
      "3                           apple day keep doctor away\n",
      "4                       nothing happens something move\n",
      "5    ten million three hundred three strip bat hang...\n",
      "6                                                 like\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "CORPUS = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"A king's strength also includes his allies\",\n",
    "    \"History is written by the victors\",\n",
    "    \"An apple a day keeps the doctor away\",\n",
    "    \"Nothing happens until something moves\",\n",
    "    \"The 10,000,303 striped bats    aren't hanging on their feet for best.\",\n",
    "    \"I did not like it\"\n",
    "    ]\n",
    "\n",
    "# Create a Pandas series \n",
    "s = pd.Series(CORPUS) \n",
    "\n",
    "# Obtain pre processed series\n",
    "preprocessed_series = text_pipeline.preprocess(s)\n",
    "print(preprocessed_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d9fd822-2b8b-40c5-a146-8f62421a6e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i hated it\n",
      "[('hated', 'VBN')]\n",
      "v\n",
      "hat\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\" Map POS tag to first character lemmatize() accepts\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word : str\n",
    "        The word that needs its Tag gleaned\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tag\n",
    "        The tag associated for the word\n",
    "\n",
    "    \"\"\"\n",
    "    print(nltk.pos_tag([word]))\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    print(tag)\n",
    "    tag_dict = {\"j\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "input_text = \"I hated it\"\n",
    "\n",
    "# Remove whitespace\n",
    "transformed_text = Text_Pipeline.remove_whitespace(input_text)\n",
    "\n",
    "# Make the text lower case\n",
    "transformed_text = transformed_text.lower()\n",
    "\n",
    "# Expand Contractions\n",
    "transformed_text = Text_Pipeline.expand_contractions(transformed_text)\n",
    "print(transformed_text)\n",
    "# Remove punctuation and convert numbers \n",
    "transformed_text = Text_Pipeline.remove_punctuation(transformed_text)\n",
    "\n",
    "filtered_words = Text_Pipeline.remove_stopwords(transformed_text)\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "transformed_text = ' '.join([lemmatizer.lemmatize(w, pos=get_wordnet_pos(w)) for w in filtered_words])\n",
    "\n",
    "print(transformed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get matrix using BOW\n",
    "matrix, column_names = text_pipeline.encode(preprocessed_series, 'BOW')\n",
    "\n",
    "result = pd.DataFrame(\n",
    "    data=matrix.toarray(), \n",
    "    index=preprocessed_series.values, \n",
    "    columns=column_names\n",
    ")\n",
    "\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfcc7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get matrix using TF-IDF\n",
    "matrix, column_names = text_pipeline.encode(preprocessed_series, 'TFIDF')\n",
    "\n",
    "result = pd.DataFrame(\n",
    "    data=matrix.toarray(), \n",
    "    index=preprocessed_series.values, \n",
    "    columns=column_names\n",
    ")\n",
    "\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def9470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get matrix using Word to Vector\n",
    "matrix = text_pipeline.encode(preprocessed_series, 'WordToVec')\n",
    "\n",
    "result = pd.DataFrame(\n",
    "    data=matrix.vectors, \n",
    "    index=matrix.key_to_index.keys()\n",
    ")\n",
    "\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75015e0-e21c-439f-8ffc-6970fd82610b",
   "metadata": {},
   "source": [
    "We will now apply a model to it using Large Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a37e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    #sentiment_analyzer = pipeline('sentiment-analysis', model=model_id)\n",
    "    result = sentiment_pipe(text)\n",
    "    return result[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee7923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyze the sentiment of a few sentences\n",
    "amazon_reviews = [\n",
    "    \"My kiddos liked it!\",\n",
    "    \"Amazon, please buy the show! I'm hooked!\",\n",
    "]\n",
    "\n",
    "#amazon_reviews = df1['text'].values\n",
    "\n",
    "# Analyze sentiment for each news headline\n",
    "sentiments = [analyze_sentiment(review) for review in amazon_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8142c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f5e53-c2fd-4b0b-8d1b-567b933b5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(amazon_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b97d29-687e-457c-b647-c1dc84577eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(preprocessed_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59bc9e-8f0b-46ab-8c52-9e568c1e6269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
